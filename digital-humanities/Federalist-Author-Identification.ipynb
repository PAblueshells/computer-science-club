{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federalist Papers Authorship Attribution\n",
    "Code by [Jon Luca](https://github.com/jonluca/Federalist-Papers-NLP), modified and annotated by Sarah Chen based off of JonLuca's 2018 [blog post](https://blog.jonlu.ca/posts/the-federalist-papers-author-identification-through-k-means-clustering).\n",
    "\n",
    "For 1/27 meeting on digital humanities (accompanying slides [here](https://docs.google.com/presentation/d/1VShMCNWyJQcMZM_6OtLe9dVeZt0WZezXYvcbgY103z0/edit?usp=sharing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sarahchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import glob\n",
    "import os\n",
    "import ntpath\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all papers from the corresponding subfolders and contenate text (accessible via Gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = r\"./papers/\"\n",
    "hamilton = sorted(glob.glob(os.path.join(papers, \"hamilton/*\")))\n",
    "madison = sorted(glob.glob(os.path.join(papers, \"madison/*\")))\n",
    "disputed = sorted(glob.glob(os.path.join(papers, \"disputed/*\")))\n",
    "\n",
    "hamilton_papers = []\n",
    "for fn in hamilton:\n",
    "    with open(fn) as f:\n",
    "        hamilton_papers.append(f.read().replace('\\n', ' ').replace('\\r',''))\n",
    "hamilton_papers_all = ' '.join(hamilton_papers)\n",
    "\n",
    "madison_papers = []\n",
    "for fn in madison:\n",
    "    with open(fn) as f:\n",
    "        madison_papers.append(f.read().replace('\\n', ' ').replace('\\r',''))\n",
    "madison_papers_all = ' '.join(madison_papers)\n",
    "\n",
    "disputed_papers = []\n",
    "disputed_papers_file_names = []\n",
    "for fn in disputed:\n",
    "    with open(fn) as f:\n",
    "        disputed_papers.append(f.read().replace('\\n', ' ').replace('\\r',''))\n",
    "        disputed_papers_file_names.append(ntpath.basename(fn))\n",
    "disputed_papers_all = ' '.join(disputed_papers)\n",
    "\n",
    "known_papers_all = hamilton_papers_all + \" \" + madison_papers_all # string of all papers (text concatenated)\n",
    "known_papers = hamilton_papers + madison_papers # list of all papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some lexical features for each paper:\n",
    "* Average number of words per sentence\n",
    "* Lexical diversity score: the number of unique words / total number of words\n",
    "* ; per sentence\n",
    "* \" per sentence\n",
    "* , per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexicalFeatures(papers, all_papers):\n",
    "    \"\"\"\n",
    "    Compute feature vectors for word and punctuation features\n",
    "    \"\"\"\n",
    "    num_papers = len(papers)\n",
    "    # for every paper, calculate 2 lexical features and 3 punctuation features\n",
    "    fvs_lexical = np.zeros((len(papers), 2), np.float64)\n",
    "    fvs_punct = np.zeros((len(papers), 3), np.float64)\n",
    "    for e, single_paper_text in enumerate(papers):\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        tokens = nltk.word_tokenize(single_paper_text.lower())\n",
    "        words = word_tokenizer.tokenize(single_paper_text.lower())\n",
    "        sentences = sentence_tokenizer.tokenize(single_paper_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s)) for s in sentences])\n",
    "\n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 1] = len(vocab) / float(len(words))\n",
    "\n",
    "        # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(';') / float(len(sentences))\n",
    "        fvs_punct[e, 1] = tokens.count('\"') / float(len(sentences))\n",
    "        fvs_punct[e, 2] = tokens.count(',') / float(len(sentences))\n",
    "        \n",
    "\n",
    "    # apply whitening to decorrelate the features (a form of normalization, scaling each feature by std)\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)\n",
    "\n",
    "    return fvs_lexical, fvs_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the frequencies of parts of speech (POS) within each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyntacticFeatures(papers, all_papers):\n",
    "\n",
    "    def token_to_pos(paper):\n",
    "        \"\"\"convert a paper to the POS tags for each word\"\"\"\n",
    "        tokens = nltk.word_tokenize(paper)\n",
    "        return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "    paper_pos = [token_to_pos(paper) for paper in papers]\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    # counts of how many times each POS appears in a given paper\n",
    "    fvs_syntax = np.array([[paper.count(pos) for pos in pos_list] for paper in paper_pos]).astype(np.float64)\n",
    "\n",
    "    # divide the POS counts for row by number of tokens in the paper to yield POS frequencies\n",
    "    fvs_syntax /= np.c_[np.array([len(paper) for paper in paper_pos])]\n",
    "\n",
    "    return fvs_syntax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster papers using the generated features with K-means clustering. Very quick [explanation of K-means](https://en.wikitolearn.org/Course:Machine_Learning_for_Humans/Unsupervised_Learning/K-Means_Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictAuthors(fvs):\n",
    "    km = KMeans(n_clusters=2, init='k-means++', n_init=100, max_iter=300, verbose=0)\n",
    "    km.fit(fvs)\n",
    "    return km\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it all together. Call the feature-generation functions we have defined on both the papers with known and disputed authorship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/scipy/cluster/vq.py:139: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "known_set = list(LexicalFeatures(known_papers, known_papers_all))\n",
    "known_set.append(SyntacticFeatures(known_papers, known_papers_all))\n",
    "\n",
    "disputed_set = list(LexicalFeatures(disputed_papers, disputed_papers_all))\n",
    "disputed_set.append(SyntacticFeatures(disputed_papers, disputed_papers_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit 3 different K-means models to the data for the lexical (vocab), lexical based on punctionation, and syntactical (based on part of speech frequencies) feature sets we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [PredictAuthors(fvs) for fvs in known_set] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the 3 sets of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "results.append([classifications[0].predict(disputed_set[0]),\"Lexical Features\"]) # Predict results of Lexical Features\n",
    "results.append([classifications[1].predict(disputed_set[1]),\"Lexical Features - Punctuation\"]) # Predict results of Lexical Features, Punctuation\n",
    "results.append([classifications[2].predict(disputed_set[2]),\"Syntactic Features\"]) # Predict results of their syntactic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hamilton', 'Hamilton', 'Madison', 'Madison', 'Madison', 'Madison', 'Madison', 'Madison', 'Madison', 'Madison', 'Hamilton', 'Madison']\n",
      "['Hamilton', 'Hamilton', 'Hamilton', 'Hamilton', 'Madison', 'Madison', 'Madison', 'Hamilton', 'Hamilton', 'Hamilton', 'Madison', 'Madison']\n",
      "['Madison', 'Madison', 'Madison', 'Madison', 'Madison', 'Hamilton', 'Madison', 'Madison', 'Madison', 'Madison', 'Madison', 'Madison']\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for i in range(len(classifications)): # for each data set\n",
    "    # Extract the label of the first paper\n",
    "    # We know for a fact Hamilton wrote the first paper, so we can label the cluster containing paper #1 as his\n",
    "    hamilton = classifications[i].labels_[0] \n",
    "    \n",
    "    # go through all papers\n",
    "    individual_classifier_results = []\n",
    "    for j in range(len(results[i][0])):\n",
    "        if results[i][0][j] == hamilton: \n",
    "            individual_classifier_results.append(\"Hamilton\")\n",
    "        else:\n",
    "            individual_classifier_results.append(\"Madison\")\n",
    "    print(individual_classifier_results)\n",
    "    all_results.append(individual_classifier_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexical Features</th>\n",
       "      <th>Lexical Features - Punctuation</th>\n",
       "      <th>Syntactic Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper 49</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 50</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 51</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 52</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 53</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Madison</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 54</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Madison</td>\n",
       "      <td>Hamilton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 55</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Madison</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 56</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 57</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 58</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 62</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>Madison</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 63</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Madison</td>\n",
       "      <td>Madison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexical Features Lexical Features - Punctuation Syntactic Features\n",
       "paper 49         Hamilton                       Hamilton            Madison\n",
       "paper 50         Hamilton                       Hamilton            Madison\n",
       "paper 51          Madison                       Hamilton            Madison\n",
       "paper 52          Madison                       Hamilton            Madison\n",
       "paper 53          Madison                        Madison            Madison\n",
       "paper 54          Madison                        Madison           Hamilton\n",
       "paper 55          Madison                        Madison            Madison\n",
       "paper 56          Madison                       Hamilton            Madison\n",
       "paper 57          Madison                       Hamilton            Madison\n",
       "paper 58          Madison                       Hamilton            Madison\n",
       "paper 62         Hamilton                        Madison            Madison\n",
       "paper 63          Madison                        Madison            Madison"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorship_assignments = pd.DataFrame(np.array(all_results).T, columns=['Lexical Features','Lexical Features - Punctuation','Syntactic Features'])\n",
    "authorship_assignments.index = ['paper '+str(name.split('.')[0]) for name in disputed_papers_file_names]\n",
    "authorship_assignments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
